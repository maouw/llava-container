This container provides a convenient way to run LLaVA. In addition to the LLaVA
module, it includes the commands:
  - `llava-run`, a command-line wrapper for LLaVA inference
  - `hyak-llava-web`, a wrapper to launch the gradio web interface and issue an
    SSH connection string you can copy to open a tunnel to your own computer.
 
To run LLaVA with the `llava-run` script, use the following command:
  apptainer run --nv --writable-tmpfs \
    oras://ghcr.io/<%= "${GITHUB_REPOSITORY:-uw-psych/llava-container}" %>/llava-container:latest \
    llava-run [llava-run arguments]

You must pass the "--nv" flag to enable GPU support.

Depending on your intended use, you may also want to pass the "--bind" flag
to mount a directory from the host system into the container.

To specify a directory to use for the HuggingFace model cache and enable access
to /gscratch, use the following command:
  apptainer run --nv --writable-tmpfs \
    --env HUGGINGFACE_HUB_CACHE=/path/to/cache \
    --bind /gscratch \
    oras://ghcr.io/<%= "${GITHUB_REPOSITORY:-uw-psych/llava-container}" %>/llava-container:latest \
    llava-run [llava-run arguments]


The following describes the usage of this script:

  llava-run [-h] [--model-path MODEL_PATH] [--model-base MODEL_BASE]
    --image-file IMAGE (--query QUERY | --chat) [--conv-mode CONV_MODE]
    [--sep SEP] [--temperature TEMPERATURE] [--top_p N]
    [--num_beams NUM_BEAMS] [--max_new_tokens N]
    [--load-8bit] [--load-4bit] [--device DEVICE]
    [--hf-cache-dir HF_CACHE_DIR]

  options:
    -h, --help            show this help message and exit
    --model-path MODEL_PATH     Model path
    --model-base MODEL_BASE     Model base
    --image-file IMAGE_FILE     Image file or URL
    --query QUERY               Query
	--chat						Chat mode 
    --conv-mode CONV_MODE       Conversation mode
    --sep SEP                   Separator for image files
    --temperature TEMPERATURE   Temperature
    --top_p TOP_P               Top p
    --num_beams NUM_BEAMS       Number of beams
    --max_new_tokens MAX_NEW_TOKENS Max new tokens
    --load-8bit                 Load 8bit model
    --load-4bit                 Load 4bit model
    --device DEVICE             cuda or cpu
    --hf-cache-dir HF_CACHE_DIR HuggingFace cache directory
  
  For details on the arguments, see the LLaVA documentation and the usage infor-
  mation for llava.eval.run_llava and llava.serve.cli.
